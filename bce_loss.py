"""
Binary Cross Entropy Loss Implementation for ImageNet Classification

Based on research findings that PyTorch's built-in BCE loss functions don't work well
out-of-the-box for ImageNet classification. This implementation addresses the issues:

1. Default reduction="mean" averages over all samples and classes, resulting in 
   infinitesimal loss values that don't allow learning with traditional learning rates
2. Need proper bias initialization to -log(n_classes) for optimal performance

Reference: Wightman et al. timm recipes and Kornblith et al. initialization practices
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Union


class ImageNetBCELoss(nn.Module):
    """
    Binary Cross Entropy Loss optimized for ImageNet classification.
    
    Key improvements over PyTorch's default BCE:
    1. Uses reduction="sum" and divides by batch size for proper loss scaling
    2. Handles multi-class to binary conversion correctly
    3. Provides reasonable loss magnitudes for effective learning
    
    Args:
        num_classes (int): Number of classes in the dataset
        label_smoothing (float, optional): Label smoothing factor (default: 0.0)
        pos_weight (Optional[torch.Tensor], optional): Positive class weights
        reduction (str): Reduction method - should be "sum" for this implementation
    """
    
    def __init__(
        self, 
        num_classes: int, 
        label_smoothing: float = 0.0,
        pos_weight: Optional[torch.Tensor] = None,
        reduction: str = "sum"
    ):
        super().__init__()
        
        if reduction != "sum":
            raise ValueError("ImageNetBCELoss requires reduction='sum' for proper scaling")
            
        self.num_classes = num_classes
        self.label_smoothing = label_smoothing
        self.reduction = reduction
        
        # Store pos_weight as parameter if provided
        if pos_weight is not None:
            self.register_buffer('pos_weight', pos_weight)
        else:
            self.pos_weight = None
    
    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of BCE loss for ImageNet classification.
        
        Args:
            input (torch.Tensor): Model predictions (logits) of shape (N, C)
            target (torch.Tensor): Target class indices of shape (N,)
            
        Returns:
            torch.Tensor: Scaled BCE loss
        """
        batch_size = input.size(0)
        
        # Convert multi-class targets to binary format
        # Create one-hot encoding for targets
        target_one_hot = F.one_hot(target, num_classes=self.num_classes).float()
        
        # Apply label smoothing if specified
        if self.label_smoothing > 0:
            # Smooth the one-hot targets
            smooth_target = target_one_hot * (1 - self.label_smoothing) + \
                          self.label_smoothing / self.num_classes
        else:
            smooth_target = target_one_hot
        
        # Convert logits to probabilities using sigmoid for BCE
        # Note: For multi-class, we use sigmoid instead of softmax
        probs = torch.sigmoid(input)
        
        # Calculate BCE loss with proper scaling
        if self.pos_weight is not None:
            # Apply positive class weights
            loss = F.binary_cross_entropy(
                probs, 
                smooth_target, 
                weight=self.pos_weight,
                reduction='none'
            )
        else:
            loss = F.binary_cross_entropy(
                probs, 
                smooth_target, 
                reduction='none'
            )
        
        # Sum over all elements (samples and classes)
        loss = loss.sum()
        
        # Divide by batch size to get proper loss scaling
        # This ensures loss magnitudes are similar to CrossEntropyLoss
        loss = loss / batch_size
        
        return loss


class ImageNetBCEWithLogitsLoss(nn.Module):
    """
    BCE with Logits Loss optimized for ImageNet classification.
    
    This version combines sigmoid and BCE in a numerically stable way,
    similar to PyTorch's BCEWithLogitsLoss but with proper scaling.
    
    Args:
        num_classes (int): Number of classes in the dataset
        label_smoothing (float, optional): Label smoothing factor (default: 0.0)
        pos_weight (Optional[torch.Tensor], optional): Positive class weights
        reduction (str): Reduction method - should be "sum" for this implementation
    """
    
    def __init__(
        self, 
        num_classes: int, 
        label_smoothing: float = 0.0,
        pos_weight: Optional[torch.Tensor] = None,
        reduction: str = "sum"
    ):
        super().__init__()
        
        if reduction != "sum":
            raise ValueError("ImageNetBCEWithLogitsLoss requires reduction='sum' for proper scaling")
            
        self.num_classes = num_classes
        self.label_smoothing = label_smoothing
        self.reduction = reduction
        
        # Store pos_weight as parameter if provided
        if pos_weight is not None:
            self.register_buffer('pos_weight', pos_weight)
        else:
            self.pos_weight = None
    
    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of BCE with logits loss for ImageNet classification.
        
        Args:
            input (torch.Tensor): Model predictions (logits) of shape (N, C)
            target (torch.Tensor): Target class indices of shape (N,)
            
        Returns:
            torch.Tensor: Scaled BCE with logits loss
        """
        batch_size = input.size(0)
        
        # Convert multi-class targets to binary format
        target_one_hot = F.one_hot(target, num_classes=self.num_classes).float()
        
        # Apply label smoothing if specified
        if self.label_smoothing > 0:
            smooth_target = target_one_hot * (1 - self.label_smoothing) + \
                          self.label_smoothing / self.num_classes
        else:
            smooth_target = target_one_hot
        
        # Use BCE with logits (numerically stable)
        if self.pos_weight is not None:
            loss = F.binary_cross_entropy_with_logits(
                input, 
                smooth_target, 
                weight=self.pos_weight,
                reduction='none'
            )
        else:
            loss = F.binary_cross_entropy_with_logits(
                input, 
                smooth_target, 
                reduction='none'
            )
        
        # Sum over all elements and divide by batch size
        loss = loss.sum() / batch_size
        
        return loss


def initialize_bce_bias(model: nn.Module, num_classes: int) -> None:
    """
    Initialize the final layer bias for BCE loss according to research findings.
    
    Based on Kornblith et al.'s practice: initialize logit biases to -log(n_classes)
    such that initial outputs are roughly equal to 1/n_classes.
    
    This prevents the ~0.8% accuracy drop compared to using traditional cross-entropy loss.
    
    Args:
        model (nn.Module): The model to initialize
        num_classes (int): Number of classes in the dataset
    """
    # Find the final linear layer (usually named 'fc' or 'classifier')
    final_layer = None
    final_layer_name = None
    
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            final_layer = module
            final_layer_name = name
    
    if final_layer is None:
        raise ValueError("No linear layer found in model for bias initialization")
    
    # Initialize bias to -log(n_classes)
    bias_init_value = -np.log(num_classes)
    nn.init.constant_(final_layer.bias, bias_init_value)
    
    print(f"âœ“ Initialized {final_layer_name} bias to -log({num_classes}) = {bias_init_value:.4f}")
    print(f"  This ensures initial outputs are roughly 1/{num_classes} = {1/num_classes:.6f}")


def create_bce_criterion(
    num_classes: int, 
    loss_type: str = "bce_with_logits",
    label_smoothing: float = 0.0,
    pos_weight: Optional[torch.Tensor] = None
) -> nn.Module:
    """
    Factory function to create BCE loss criterion for ImageNet classification.
    
    Args:
        num_classes (int): Number of classes in the dataset
        loss_type (str): Type of BCE loss ("bce" or "bce_with_logits")
        label_smoothing (float): Label smoothing factor
        pos_weight (Optional[torch.Tensor]): Positive class weights
        
    Returns:
        nn.Module: BCE loss criterion
    """
    if loss_type == "bce":
        return ImageNetBCELoss(
            num_classes=num_classes,
            label_smoothing=label_smoothing,
            pos_weight=pos_weight
        )
    elif loss_type == "bce_with_logits":
        return ImageNetBCEWithLogitsLoss(
            num_classes=num_classes,
            label_smoothing=label_smoothing,
            pos_weight=pos_weight
        )
    else:
        raise ValueError(f"Unknown loss_type: {loss_type}. Choose 'bce' or 'bce_with_logits'")


def compare_loss_scaling(
    model: nn.Module, 
    input_tensor: torch.Tensor, 
    target_tensor: torch.Tensor,
    num_classes: int
) -> dict:
    """
    Compare loss scaling between CrossEntropy and BCE implementations.
    
    This utility helps validate that BCE loss values are in a reasonable range
    compared to CrossEntropy loss.
    
    Args:
        model (nn.Module): The model to test
        input_tensor (torch.Tensor): Input batch
        target_tensor (torch.Tensor): Target batch
        num_classes (int): Number of classes
        
    Returns:
        dict: Comparison of loss values
    """
    model.eval()
    
    with torch.no_grad():
        output = model(input_tensor)
        
        # CrossEntropy loss
        ce_loss = F.cross_entropy(output, target_tensor)
        
        # BCE losses
        bce_loss = ImageNetBCELoss(num_classes)(output, target_tensor)
        bce_logits_loss = ImageNetBCEWithLogitsLoss(num_classes)(output, target_tensor)
        
        return {
            "cross_entropy": ce_loss.item(),
            "bce": bce_loss.item(),
            "bce_with_logits": bce_logits_loss.item(),
            "bce_ratio": bce_loss.item() / ce_loss.item(),
            "bce_logits_ratio": bce_logits_loss.item() / ce_loss.item()
        }


# Example usage and testing
if __name__ == "__main__":
    # Test the BCE loss implementation
    print("Testing ImageNet BCE Loss Implementation")
    print("=" * 50)
    
    # Create dummy data
    batch_size = 32
    num_classes = 1000
    input_size = 224
    
    # Dummy model output (logits)
    logits = torch.randn(batch_size, num_classes)
    targets = torch.randint(0, num_classes, (batch_size,))
    
    print(f"Batch size: {batch_size}")
    print(f"Number of classes: {num_classes}")
    print(f"Logits shape: {logits.shape}")
    print(f"Targets shape: {targets.shape}")
    print()
    
    # Test different loss functions
    print("Loss Comparison:")
    print("-" * 30)
    
    # CrossEntropy (baseline)
    ce_loss = F.cross_entropy(logits, targets)
    print(f"CrossEntropy Loss: {ce_loss.item():.6f}")
    
    # BCE Loss
    bce_loss = ImageNetBCELoss(num_classes)(logits, targets)
    print(f"BCE Loss: {bce_loss.item():.6f}")
    
    # BCE with Logits Loss
    bce_logits_loss = ImageNetBCEWithLogitsLoss(num_classes)(logits, targets)
    print(f"BCE with Logits Loss: {bce_logits_loss.item():.6f}")
    
    print()
    print("Loss Ratios (relative to CrossEntropy):")
    print(f"BCE / CrossEntropy: {bce_loss.item() / ce_loss.item():.3f}")
    print(f"BCE with Logits / CrossEntropy: {bce_logits_loss.item() / ce_loss.item():.3f}")
    
    print()
    print("âœ“ BCE loss implementation working correctly!")
    print("âœ“ Loss values are in reasonable range compared to CrossEntropy")
